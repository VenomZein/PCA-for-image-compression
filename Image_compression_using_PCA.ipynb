{
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.2",
      "file_extension": ".r",
      "codemirror_mode": "r"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 737776,
          "sourceType": "datasetVersion",
          "datasetId": 370587
        }
      ],
      "dockerImageVersionId": 29833,
      "isInternetEnabled": false,
      "language": "rmarkdown",
      "sourceType": "script",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Image compression using PCA",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Hi! In this kernel we are going to use the stadistical method PCA (Principal Component Analysis) to compress images. What is it? PCA is mathematically defined as an orthogonal linear transformation\n",
        "that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component),\n",
        "the second greatest variance on the second coordinate, and so on. In other words, we convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated\n",
        "variables called principal components (**Reference**: https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
        "\n",
        "To sum up, the aim is to explain as much data variation as possible while discarding highly correlated variables. Before moving to the image compression, let's see in detail how this statistical\n",
        "technique works.\n",
        "\n",
        "In this kernel we are going to see one interesting application of this stadistical method (image compression), but it has many others in different fields: quantitative finance\n",
        "or neuroscience, for example.\n",
        "\n",
        "# **Load libraries**\n",
        "\n",
        "First we need to load some libraries.\n",
        "\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# Load libraries\n",
        "library(tidyverse)\n",
        "library(jpeg)\n",
        "library(factoextra)\n",
        "library(knitr)\n",
        "```\n",
        "\n",
        "# **Background mathematics**\n",
        "\n",
        "To do this section I'm going to use the article \"[A tutorial on principal components analysis](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)\"\n",
        "by Lindsay I Smith as a reference.\n",
        "\n",
        "## Data {.tabset .tabset-fade .tabset-pills}\n",
        "\n",
        "Let's see an example to study how this algorithm works. The following table only contains two dimensions: $x$ and $y$. We are going to use a reduced data set to simplify the calculations and\n",
        "provide plots to show what the PCA analysis is doing at each step.\n",
        "\n",
        "<center>\n",
        "\n",
        "|  $x$   |  $y$   |\n",
        "|:-------|:-------|\n",
        "| 2.5    | 2.4    |\n",
        "| 0.5    | 0.7    |\n",
        "| 2.2    | 2.9    |\n",
        "| 1.9    | 2.2    |\n",
        "| 3.1    | 3      |\n",
        "| 2.3    | 2.7    |\n",
        "| 2      | 1.6    |\n",
        "| 1      | 1.1    |\n",
        "| 1.5    | 1.6    |\n",
        "| 1.1    | 0.9    |\n",
        "\n",
        "</center>\n",
        "\n",
        "```{r fig.align='center', message=FALSE, warning=FALSE}\n",
        "# Data points\n",
        "x <- c(2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2, 1, 1.5, 1.1)\n",
        "y <- c(2.4, 0.7, 2.9, 2.2, 3, 2.7, 1.6, 1.1, 1.6, 0.9)\n",
        "\n",
        "# Data frame\n",
        "data <- data.frame(x, y)\n",
        "\n",
        "# Scatter plot\n",
        "data %>%\n",
        "  ggplot(aes(x,y)) +\n",
        "  geom_point(size=2, shape=3, color=\"blue\") +\n",
        "  theme_bw() +\n",
        "  labs(title=\"Original data points\")\n",
        "```\n",
        "\n",
        "### Stucture\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# Structure\n",
        "str(data)\n",
        "```\n",
        "\n",
        "### Summary\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# Summary\n",
        "summary(data)\n",
        "```\n",
        "\n",
        "### First rows\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# First rows\n",
        "head(data)\n",
        "```\n",
        "\n",
        "### Last rows\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# Last rows\n",
        "tail(data)\n",
        "```\n",
        "\n",
        "## Substract the mean\n",
        "\n",
        "First, we have to subtract the mean from each of the data dimensions. All the $x$ values have $\\bar{x}$ (the mean of the $x$ values of all the data points) subtracted, and all the $y$ values\n",
        "have $\\bar{y}$ subtracted from them.\n",
        "\n",
        "So we have to compute $(x-\\bar{x})$ and $(y-\\bar{y})$, where $\\bar{x}=\\frac{\\sum_{i=1}^{n}x_i}{n}$ and $\\bar{y}=\\frac{\\sum_{i=1}^{n}y_i}{n}$.\n",
        "\n",
        "<center>\n",
        "\n",
        "|  $x$   |  $y$   | $(x-\\bar{x})$   |  $(y-\\bar{y})$   |\n",
        "|:-------|:-------|:----------------|:-----------------|\n",
        "| 2.5    | 2.4    | 0.69            |0.49              |\n",
        "| 0.5    | 0.7    | -1.31           |-1.21             |\n",
        "| 2.2    | 2.9    | 0.39            |0.99              |\n",
        "| 1.9    | 2.2    | 0.09            |0.29              |\n",
        "| 3.1    | 3      | 1.29            |1.09              |\n",
        "| 2.3    | 2.7    | 0.49            |0.79              |\n",
        "| 2      | 1.6    | 0.19            |-0.31             |\n",
        "| 1      | 1.1    | -0.81           |-0.81             |\n",
        "| 1.5    | 1.6    | -0.31           |-0.31             |\n",
        "| 1.1    | 0.9    | -0.71           |-1.01             |\n",
        "\n",
        "</center>\n",
        "\n",
        "This produces a data set whose mean is zero.\n",
        "\n",
        "## Calculate the covariance matrix\n",
        "\n",
        "The aim of the covariance matrix calculation is usually to see if there is any relationship between the dimensions. The covariance matrix for this 2 dimensional data set can be expressed as\n",
        "\n",
        "\\[\n",
        "C=\n",
        "\\begin{pmatrix}\n",
        "    cov(x, x) & cov(x, y) \\\\\n",
        "    cov(y, x) & cov(y, y)\n",
        "\\end{pmatrix}\n",
        "\\]\n",
        "\n",
        "where\n",
        "\n",
        "* $cov(x, y)=cov(y, x)= \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}$\n",
        "\n",
        "* $cov(x, x)= \\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(x_i-\\bar{x})}{n-1}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}{n-1}= var(x)$\n",
        "\n",
        "* $cov(y, y)= \\frac{\\sum_{i=1}^{n}(y_i-\\bar{y})(y_i-\\bar{y})}{n-1}= \\frac{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}{n-1}= var(y)$\n",
        "\n",
        "Covariance is always measured between 2 dimensions. If you calculate the covariance between one dimension and itself, you get the\n",
        "variance. Let's make all the calculations:\n",
        "\n",
        "<center>\n",
        "\n",
        "|  $x$   |  $y$   | $(x-\\bar{x})$   |  $(y-\\bar{y})$   |$(x-\\bar{x})(y-\\bar{y})$ |$(x-\\bar{x})^2$   |$(y-\\bar{y})^2$ |\n",
        "|:-------|:-------|:----------------|:-----------------|:------------------------|:-----------------|:---------------|\n",
        "| 2.5    | 2.4    | 0.69            |0.49              | 0.3381                  |0.4761            |0.2401          |\n",
        "| 0.5    | 0.7    | -1.31           |-1.21             |1.5851                   |1.7161            |1.4641          |\n",
        "| 2.2    | 2.9    | 0.39            |0.99              |0.3861                   |0.1521            |0.9801          |\n",
        "| 1.9    | 2.2    | 0.09            |0.29              |0.0261                   |0.0081            |0.0841          |\n",
        "| 3.1    | 3      | 1.29            |1.09              |1.4061                   |1.6641            |1.1881          |\n",
        "| 2.3    | 2.7    | 0.49            |0.79              |0.3871                   |0.2401            |0.6241          |\n",
        "| 2      | 1.6    | 0.19            |-0.31             |-0.0589                  |0.0361            |0.0961          |\n",
        "| 1      | 1.1    | -0.81           |-0.81             |0.6561                   |0.6561            |0.6561          |\n",
        "| 1.5    | 1.6    | -0.31           |-0.31             |0.0961                   |0.0961            |0.0961          |\n",
        "| 1.1    | 0.9    | -0.71           |-1.01             |0.7171                   |0.5041            |1.0201          |\n",
        "\n",
        "</center>\n",
        "\n",
        "With these values we can obtain easily the covariance matrix:\n",
        "\n",
        "\\[\n",
        "C=\n",
        "\\begin{pmatrix}\n",
        "    0.6165 & 0.6154 \\\\\n",
        "    0.6154 & 0.7165\n",
        "\\end{pmatrix}\n",
        "\\]\n",
        "\n",
        "We should expect that both the $x$ and $y$ variable increase together, since the non-diagonal elements in this covariance matrix are positive.\n",
        "\n",
        "## Eigenvectors and eigenvalues\n",
        "\n",
        "The eigenvectors and eigenvalues of a covariance matrix represent the “core” of a PCA: the eigenvectors determine the directions of the new feature space, and the eigenvalues determine their\n",
        "magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes. The eigenvector with the highest eigenvalue is, therefore, the first principal component.\n",
        "\n",
        "The equation to find the eigenvectors and eigenvalues in our problem is the following:\n",
        "\n",
        "\\[\n",
        "\\begin{pmatrix} a  &  c \\\\ c  &  b \\end{pmatrix} \\begin{pmatrix}  x \\\\  y  \\end{pmatrix} = \\lambda \\begin{pmatrix} x \\\\  y \\end{pmatrix}\n",
        "\\]\n",
        "\n",
        "where $a=0.6165$, $b=0.7165$ and $c=0.6154$.\n",
        "\n",
        "We can use the function [`eigen()`](https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/eigen) to calculate the solution. If you want more information about how to compute\n",
        "the eigenvalues and eigenvectors, check [here](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).\n",
        "\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# Eigenvectors and eigenvalues calculation\n",
        "eigen <- eigen(data.frame(c(0.616555556, 0.615444444),\n",
        "                          c(0.615444444, 0.716555556)))\n",
        "\n",
        "# Eigenvectors\n",
        "eigen$vectors\n",
        "\n",
        "# Eigenvalues\n",
        "eigen$values\n",
        "```\n",
        "\n",
        "So the solutions are\n",
        "\n",
        "\\[\n",
        "eigenvalues=\\begin{pmatrix} 1.2840277  \\\\ 0.0490834   \\end{pmatrix}\n",
        "\\]\n",
        "\n",
        "\\[\n",
        "eigenvectors=\\begin{pmatrix} 0.6778734   &  -0.7351787 \\\\ 0.7351787  &  0.6778734 \\end{pmatrix}\n",
        "\\]\n",
        "\n",
        "The number of eigenvalues and eigenvectors that exits is equal to the number of dimensions the data set has. In our example we have 2 variables, so the data set is two-dimensional.\n",
        "That means that there are two eigenvectors and eigenvalues.\n",
        "\n",
        "```{r fig.align='center', message=FALSE, warning=FALSE}\n",
        "# Points with the mean substracted\n",
        "xMeanSubstracted <- x - mean(x)\n",
        "yMeanSubstracted <- y - mean(y)\n",
        "data2 <- data.frame(xMeanSubstracted, xMeanSubstracted)\n",
        "\n",
        "# Eigenvectors functions\n",
        "fun.1 <- function(x) (0.7351787/0.6778734)*x\n",
        "fun.2 <- function(x) (0.6778734/-0.7351787)*x\n",
        "\n",
        "# Scatter plot with the eigenvectors overlayed\n",
        "data2 %>%\n",
        "  ggplot(aes(xMeanSubstracted, yMeanSubstracted)) +\n",
        "  geom_point(size=2, shape=3, color=\"blue\") +\n",
        "  stat_function(fun=fun.1, linetype=\"dashed\") +\n",
        "  stat_function(fun=fun.2, linetype=\"dashed\") +\n",
        "  theme_bw() +\n",
        "  xlim(-1.5, 1.5) +\n",
        "  labs(title=\"Mean adjusted data with eigenvectors overlayed\",\n",
        "       x=\"x\", y=\"y\") +\n",
        "  annotate(\"text\", x=c(-1.1, 0.9), y=c(1.5, 1.5),\n",
        "           label=c(\"Second Component\", \"First Component\"))\n",
        "```\n",
        "\n",
        "The eigenvector with the highest eigenvalue is the principle component of the data set.\n",
        "\n",
        "## Choosing components\n",
        "\n",
        "Once eigenvectors are found from the covariance matrix, the next step is to order them by eigenvalue, highest to lowest. This gives you the components in order of significance.\n",
        "You can decide to ignore the components of lesser significance, so the final data set will have less dimensions than the original.\n",
        "\n",
        "In order to decide, let's analize the percentage of variances explained by each principal component.\n",
        "\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# Principal Component Analysis\n",
        "pca <- prcomp(data, center=TRUE)\n",
        "\n",
        "# We can visualize the eigenvectors with the function fviz_eig()\n",
        "# Documentation: https://www.rdocumentation.org/packages/factoextra/versions/1.0.5/topics/eigenvalue\n",
        "# Unfortunately, I get an error when I execute this function on Kaggle\n",
        "# fviz_eig(pca)\n",
        "\n",
        "# Cumulative proportion\n",
        "summary(pca)\n",
        "```\n",
        "\n",
        "PC1 explains 96% of the total variance, so we can discard the second component.\n",
        "\n",
        "## Deriving the new data set\n",
        "\n",
        "Once we have chosen the components (eigenvectors) that we wish to keep in our data, we simply take the transpose of the vector and multiply it on the left of the original data set, transposed.\n",
        "\n",
        "\\[\n",
        "FinalData=RowFeatureVector^T \\,×\\, RowDataAdjust^T\n",
        "\\]\n",
        "\n",
        "```{r fig.align='center', message=FALSE, warning=FALSE}\n",
        "# Data expressed in terms of our 2 eigenvectors\n",
        "dataNewAxes <- as.data.frame(t(t(eigen$vectors) %*% rbind(x - mean(x), y - mean(y))))\n",
        "names(dataNewAxes) <- c(\"x\", \"y\")\n",
        "\n",
        "# New data set\n",
        "dataNewAxes\n",
        "\n",
        "# Visualization\n",
        "dataNewAxes %>%\n",
        "  ggplot(aes(x, y)) +\n",
        "  geom_point(size=2, shape=3, color=\"blue\") +\n",
        "  theme_bw() +\n",
        "  labs(title=\"Data expressed in terms of our 2 eigenvectors\",\n",
        "       x=\"First Component\", y=\"Second Component\")\n",
        "```\n",
        "\n",
        "We have changed our data from being in terms of the axes `x` and `y` and now they are in terms of our 2 eigenvectors. This plot is basically the original data,\n",
        "rotated so that the eigenvectors are the axes.\n",
        "\n",
        "## Getting the old data back\n",
        "\n",
        "If we took all the eigenvectors in our transformation will we get exactly the original data back. If we have reduced the number of eigenvectors in the final transformation,\n",
        "then the retrieved data has lost some information. We are going to use the first principal component (96% of the total variance). So, how do we get the original data back?\n",
        "We can use the following formula:\n",
        "\n",
        "\\[\n",
        "RowOriginalData=(RowFeatureVector^T \\,×\\, FinalData) +  OriginalMean\n",
        "\\]\n",
        "\n",
        "```{r fig.align='center', message=FALSE, warning=FALSE}\n",
        "# Reconstructed data using only the first principal component\n",
        "as.data.frame(t(t(pca$x[, 1] %*% t(pca$rotation[, 1])) + pca$center)) %>%\n",
        "ggplot(aes(x, y)) +\n",
        "  geom_point(size=2, shape=3, color=\"blue\") +\n",
        "  theme_bw() +\n",
        "  labs(title=\"Original data restored using only a single eigenvector\")\n",
        "```\n",
        "\n",
        "The variation along the principle eigenvector has been kept, but the variation along the other component (the other eigenvector that we left out) has gone.\n",
        "\n",
        "Congratulations! We are at the end of this introductory example and have performed PCA from scratch to reduce the dimensionality of our data.\n",
        "\n",
        "# **Image compression**\n",
        "\n",
        "Now that we understand better the PCA method, we can show a more interesting example related to image compression. We are going to reconstruct an image using increasing amounts of principal\n",
        "components. We will see that as the number of principal components increase, the more representative of the original image the reconstruction becomes. How many PCs are enough to compress the\n",
        "image while maintaining a good quality?\n",
        "\n",
        "## Loading image {.tabset .tabset-fade .tabset-pills}\n",
        "\n",
        "Let's read the image first.\n",
        "\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# Read image\n",
        "image <- readJPEG(\"../input/image-compression/image.jpg\")\n",
        "```\n",
        "\n",
        "Let’s get an idea of what we’re working with. The image is now represented as three 432x768 matrices as an array with each matrix corresponding to the RGB color value scheme.\n",
        "\n",
        "### Stucture\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# Structure\n",
        "str(image)\n",
        "```\n",
        "\n",
        "### Dimensions\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# Structure\n",
        "dim(image)\n",
        "```\n",
        "\n",
        "## PCA\n",
        "\n",
        "We are going to break down each color scheme into three data frames.\n",
        "\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# RGB color matrices\n",
        "rimage <- image[,,1]\n",
        "gimage <- image[,,2]\n",
        "bimage <- image[,,3]\n",
        "```\n",
        "\n",
        "Then we can apply the PCA separately for each color scheme.\n",
        "\n",
        "```{r message=FALSE, warning=FALSE}\n",
        "# PCA for each color scheme\n",
        "pcar <- prcomp(rimage, center=FALSE)\n",
        "pcag <- prcomp(gimage, center=FALSE)\n",
        "pcab <- prcomp(bimage, center=FALSE)\n",
        "\n",
        "# PCA objects into a list\n",
        "pcaimage <- list(pcar, pcag, pcab)\n",
        "```\n",
        "\n",
        "## Scree plot and cumulative variation plot\n",
        "\n",
        "In the following visualization we can study the percentage of variances explained by each principal component.\n",
        "\n",
        "```{r fig.align='center', message=FALSE, warning=FALSE}\n",
        "# Data frame for easier plotting\n",
        "df <- data.frame(scheme=rep(c(\"R\", \"G\", \"B\"), each=nrow(image)),\n",
        "                 index=rep(1:nrow(image), 3),\n",
        "                 var=c(pcar$sdev^2,\n",
        "                       pcag$sdev^2,\n",
        "                       pcab$sdev^2))\n",
        "\n",
        "# Reorder of factors\n",
        "df$scheme <- factor(df$scheme, levels(df$scheme)[c(3,2,1)])\n",
        "\n",
        "# Scree plot\n",
        "df %>%\n",
        "  group_by(scheme) %>%\n",
        "  mutate(propvar=100*var/sum(var)) %>%\n",
        "  ungroup() %>%\n",
        "  ggplot(aes(x=index, y=propvar, fill=scheme)) +\n",
        "  geom_bar(stat=\"identity\") +\n",
        "  geom_line() +\n",
        "  labs(title=\"Scree plot\", x=\"Principal Component\",\n",
        "       y=\"% of Variance\") +\n",
        "  scale_x_continuous(limits=c(0, 20)) +\n",
        "  facet_wrap(~scheme) +\n",
        "  theme_bw() +\n",
        "  theme(legend.title=element_blank(),\n",
        "        legend.position=\"bottom\")\n",
        "```\n",
        "\n",
        "With only the first principal component we can explain more than 70% of the total variance. Maybe the visualization is better if we plot the cumulative variation. Let's see!\n",
        "\n",
        "```{r fig.align='center', message=FALSE, warning=FALSE}\n",
        "# Cumulative variation plot\n",
        "df %>%\n",
        "  group_by(scheme) %>%\n",
        "  mutate(propvar=100*var/sum(var)) %>%\n",
        "  mutate(cumsum=cumsum(propvar)) %>%\n",
        "  ungroup() %>%\n",
        "  ggplot(aes(x=index, y=cumsum, fill=scheme)) +\n",
        "  geom_bar(stat=\"identity\") +\n",
        "  geom_line() +\n",
        "  labs(title=\"Cumulative proportion of variance explained\",\n",
        "       x=\"Principal Component\", y=\"Cumulative % of Variance\") +\n",
        "  scale_x_continuous(limits=c(0, 20)) +\n",
        "  facet_wrap(~scheme) +\n",
        "  theme_bw() +\n",
        "  theme(legend.title=element_blank(),\n",
        "        legend.position=\"bottom\")\n",
        "```\n",
        "\n",
        "## Image reconstruction\n",
        "\n",
        "In the following code we reconstruct the image four times: using 2, 30, 200 and 300 principal components. As more principal components are used, the more the variance (information) is described.\n",
        "The first few principal components will have the most drastic change in quality while the last few components will not make much if any, difference to quality.\n",
        "\n",
        "```{r fig.align='center', eval=FALSE, message=FALSE, warning=FALSE}\n",
        "# PCs values\n",
        "pcnum <- c(2, 30, 200, 300)\n",
        "\n",
        "# Reconstruct the image four times\n",
        "for(i in pcnum){\n",
        "    pca.img <- sapply(pcaimage, function(j){\n",
        "      compressed.img <- j$x[, 1:i] %*% t(j$rotation[, 1:i])\n",
        "    }, simplify='array')\n",
        "  writeJPEG(pca.img, paste(\"C:/Users/xviva/Desktop/Xavier/Formacion/Trabajos Kaggle/\n",
        "                           image compress/Image reconstruction with\",\n",
        "            round(i, 0), \"principal components.jpg\"))\n",
        "}\n",
        "```\n",
        "\n",
        "The code saves the four images in my local PC using the function [`writeJPEG()`](https://www.rdocumentation.org/packages/jpeg/versions/0.1-8/topics/writeJPEG). Let's see the results:\n",
        "\n",
        "<center> Image reconstruction using **2 principal components** </center>\n",
        "\n",
        "<center><img\n",
        "src=\"https://i.imgur.com/YPGHl21.jpg\">\n",
        "</center>\n",
        "\n",
        "<center> Image reconstruction using **30 principal components** </center>\n",
        "\n",
        "<center><img\n",
        "src=\"https://i.imgur.com/ELCMOSi.jpg\">\n",
        "</center>\n",
        "\n",
        "<center> Image reconstruction using **200 principal components** </center>\n",
        "\n",
        "<center><img\n",
        "src=\"https://i.imgur.com/j3GfxHF.jpg\">\n",
        "</center>\n",
        "\n",
        "<center> Image reconstruction using **300 principal components** </center>\n",
        "\n",
        "<center><img\n",
        "src=\"https://i.imgur.com/IiEWRHD.jpg\">\n",
        "</center>\n",
        "\n",
        "<center> **Original image** </center>\n",
        "\n",
        "<center><img\n",
        "src=\"https://i.imgur.com/KwY9NeU.jpg\">\n",
        "</center>\n",
        "\n",
        "By the way, the images are in [The Natural Park of Montseny](https://en.wikipedia.org/wiki/Montseny_Massif), the most recognizable natural landscape of Catalonia."
      ],
      "metadata": {
        "_uuid": "e2aff0fd-6408-4045-89a4-c8f339ad36c2",
        "_cell_guid": "9f66840e-53de-4b30-b0ea-55509dc199b6",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "mkb0MGuC6fob"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}